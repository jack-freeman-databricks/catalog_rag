{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14b680d9-1a17-4bfd-913d-f6fb9d469fd6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "#Tool-calling Agent\n",
        "\n",
        "This is an auto-generated notebook created by an AI playground export. In this notebook, you will:\n",
        "- Author a tool-calling [MLflow's `ResponsesAgent`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) that uses the OpenAI client\n",
        "- Manually test the agent's output\n",
        "- Evaluate the agent with Mosaic AI Agent Evaluation\n",
        "- Log and deploy the agent\n",
        "\n",
        "This notebook should be run on serverless or a cluster with DBR<17.\n",
        "\n",
        " **_NOTE:_**  This notebook uses the OpenAI SDK, but AI Agent Framework is compatible with any agent authoring framework, including LlamaIndex or LangGraph. To learn more, see the [Authoring Agents](https://docs.databricks.com/generative-ai/agent-framework/author-agent) Databricks documentation.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Address all `TODO`s in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0fb887d2-5ca3-4e35-8636-7d1f44150d9d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "%pip install -U -qqqq backoff databricks-openai uv databricks-agents mlflow-skinny[databricks] gepa\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4419478f-8c4a-4189-a714-ab69426a5aab",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Define the agent in code\n",
        "Below we define our agent code in a single cell, enabling us to easily write it to a local Python file for subsequent logging and deployment using the `%%writefile` magic command.\n",
        "\n",
        "For more examples of tools to add to your agent, see [docs](https://docs.databricks.com/generative-ai/agent-framework/agent-tool.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5d64c4b2-4e5e-4664-9b0d-1f17e82b2fa8",
          "showTitle": true,
          "tableResultSettingsMap": {},
          "title": "Cell 4"
        }
      },
      "outputs": [],
      "source": [
        "%%writefile agent.py\n",
        "import json\n",
        "from typing import Any, Callable, Generator, Optional\n",
        "from uuid import uuid4\n",
        "import warnings\n",
        "\n",
        "import backoff\n",
        "import mlflow\n",
        "import mlflow.genai\n",
        "import openai\n",
        "from databricks.sdk import WorkspaceClient\n",
        "from databricks_openai import UCFunctionToolkit, VectorSearchRetrieverTool\n",
        "from mlflow.entities import SpanType, Document\n",
        "from mlflow.pyfunc import ResponsesAgent\n",
        "from mlflow.types.responses import (\n",
        "    ResponsesAgentRequest,\n",
        "    ResponsesAgentResponse,\n",
        "    ResponsesAgentStreamEvent,\n",
        "    output_to_responses_items_stream,\n",
        "    to_chat_completions_input,\n",
        ")\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "from unitycatalog.ai.core.base import get_uc_function_client\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "############################################\n",
        "# Define your LLM endpoint and prompt from registry\n",
        "############################################\n",
        "LLM_ENDPOINT_NAME = \"databricks-gpt-5-mini\"\n",
        "\n",
        "# Specify the prompt name from the MLflow prompt registry\n",
        "# In Databricks, prompts are registered in Unity Catalog format: catalog.schema.promptname\n",
        "PROMPT_NAME = \"jack_demos_classic.data_catalogue_demo.catalog_rag_system_prompt\"\n",
        "\n",
        "# Load the system prompt from the prompt registry\n",
        "try:\n",
        "    # Get the latest version number first\n",
        "    client = MlflowClient()\n",
        "    response = client.search_prompt_versions(PROMPT_NAME)\n",
        "    if response.prompt_versions:\n",
        "        latest_version = max(v.version for v in response.prompt_versions)\n",
        "        # Load the latest version using mlflow.genai.load_prompt\n",
        "        prompt_obj = mlflow.genai.load_prompt(name_or_uri=PROMPT_NAME, version=latest_version)\n",
        "        SYSTEM_PROMPT = prompt_obj.template\n",
        "        print(f\"Loaded prompt '{PROMPT_NAME}' (version {prompt_obj.version}) from registry\")\n",
        "    else:\n",
        "        raise Exception(\"No versions found\")\n",
        "except Exception as e:\n",
        "    # Fallback to default prompt if registry prompt doesn't exist\n",
        "    print(f\"Warning: Could not load prompt '{PROMPT_NAME}' from registry: {e}\")\n",
        "    print(\"Using default system prompt.\")\n",
        "    SYSTEM_PROMPT = \"\"\"you are a helpful assistant, when prompted with a query search for related entries in the vector search. You report back with tables and datasets related to the user question.\"\"\"\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "## Define tools for your agent, enabling it to retrieve data or take actions\n",
        "## beyond text generation\n",
        "## To create and see usage examples of more tools, see\n",
        "## https://docs.databricks.com/generative-ai/agent-framework/agent-tool.html\n",
        "###############################################################################\n",
        "class ToolInfo(BaseModel):\n",
        "    \"\"\"\n",
        "    Class representing a tool for the agent.\n",
        "    - \"name\" (str): The name of the tool.\n",
        "    - \"spec\" (dict): JSON description of the tool (matches OpenAI Responses format)\n",
        "    - \"exec_fn\" (Callable): Function that implements the tool logic\n",
        "    - \"is_retriever\" (bool): Whether this tool is a retriever (for proper span typing)\n",
        "    \"\"\"\n",
        "\n",
        "    name: str\n",
        "    spec: dict\n",
        "    exec_fn: Callable\n",
        "    is_retriever: bool = False\n",
        "\n",
        "\n",
        "def create_tool_info(tool_spec, exec_fn_param: Optional[Callable] = None, is_retriever: bool = False):\n",
        "    tool_spec[\"function\"].pop(\"strict\", None)\n",
        "    tool_name = tool_spec[\"function\"][\"name\"]\n",
        "    udf_name = tool_name.replace(\"__\", \".\")\n",
        "\n",
        "    # Define a wrapper that accepts kwargs for the UC tool call,\n",
        "    # then passes them to the UC tool execution client\n",
        "    def exec_fn(**kwargs):\n",
        "        function_result = uc_function_client.execute_function(udf_name, kwargs)\n",
        "        if function_result.error is not None:\n",
        "            return function_result.error\n",
        "        else:\n",
        "            return function_result.value\n",
        "    return ToolInfo(name=tool_name, spec=tool_spec, exec_fn=exec_fn_param or exec_fn, is_retriever=is_retriever)\n",
        "\n",
        "\n",
        "def create_vs_tool_wrapper(vs_tool_instance):\n",
        "    \"\"\"Create a proper closure for vector search tool execution.\"\"\"\n",
        "    def execute_wrapper(**kwargs):\n",
        "        return vs_tool_instance.execute(**kwargs)\n",
        "    return execute_wrapper\n",
        "\n",
        "\n",
        "TOOL_INFOS = []\n",
        "\n",
        "# You can use UDFs in Unity Catalog as agent tools\n",
        "# TODO: Add additional tools\n",
        "UC_TOOL_NAMES = []\n",
        "\n",
        "uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES)\n",
        "uc_function_client = get_uc_function_client()\n",
        "for tool_spec in uc_toolkit.tools:\n",
        "    TOOL_INFOS.append(create_tool_info(tool_spec))\n",
        "\n",
        "\n",
        "# Use Databricks vector search indexes as tools\n",
        "# See [docs](https://docs.databricks.com/generative-ai/agent-framework/unstructured-retrieval-tools.html) for details\n",
        "\n",
        "# Use Databricks vector search indexes as tools\n",
        "# See the [Databricks Documentation](https://docs.databricks.com/generative-ai/agent-framework/unstructured-retrieval-tools.html) for details\n",
        "VECTOR_SEARCH_TOOLS = []\n",
        "VECTOR_SEARCH_TOOLS.append(\n",
        "        VectorSearchRetrieverTool(\n",
        "            index_name=\"jack_demos_classic.data_catalogue_demo.metadata_docs_index\",\n",
        "            disable_notice=True,\n",
        "            # TODO: specify index description for better agent tool selection\n",
        "            # tool_description=\"\"\n",
        "        )\n",
        "    )\n",
        "for vs_tool in VECTOR_SEARCH_TOOLS:\n",
        "    TOOL_INFOS.append(create_tool_info(vs_tool.tool, create_vs_tool_wrapper(vs_tool), is_retriever=True))\n",
        "\n",
        "\n",
        "\n",
        "class ToolCallingAgent(ResponsesAgent):\n",
        "    \"\"\"\n",
        "    Class representing a tool-calling Agent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_endpoint: str, tools: list[ToolInfo]):\n",
        "        \"\"\"Initializes the ToolCallingAgent with tools.\"\"\"\n",
        "        self.llm_endpoint = llm_endpoint\n",
        "        self.workspace_client = WorkspaceClient()\n",
        "        self.model_serving_client: OpenAI = (\n",
        "            self.workspace_client.serving_endpoints.get_open_ai_client()\n",
        "        )\n",
        "        self._tools_dict = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def get_tool_specs(self) -> list[dict]:\n",
        "        \"\"\"Returns tool specifications in the format OpenAI expects.\"\"\"\n",
        "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
        "\n",
        "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
        "        \"\"\"Executes the specified tool with the given arguments.\"\"\"\n",
        "        tool_info = self._tools_dict[tool_name]\n",
        "        \n",
        "        # Use RETRIEVER span for retrieval tools, TOOL span for others\n",
        "        span_type = SpanType.RETRIEVER if tool_info.is_retriever else SpanType.TOOL\n",
        "        \n",
        "        with mlflow.start_span(name=tool_name, span_type=span_type) as span:\n",
        "            result = tool_info.exec_fn(**args)\n",
        "            \n",
        "            # For retriever tools, set the retrieved documents as span outputs\n",
        "            if tool_info.is_retriever and result:\n",
        "                try:\n",
        "                    # Parse the result - VectorSearchRetrieverTool returns JSON string\n",
        "                    if isinstance(result, str):\n",
        "                        parsed_result = json.loads(result)\n",
        "                    else:\n",
        "                        parsed_result = result\n",
        "                    \n",
        "                    # Extract documents and convert to Document objects\n",
        "                    documents = []\n",
        "                    if isinstance(parsed_result, list):\n",
        "                        for doc in parsed_result:\n",
        "                            if isinstance(doc, dict):\n",
        "                                # Extract page_content and metadata\n",
        "                                page_content = doc.get(\"page_content\", doc.get(\"content\", doc.get(\"text\", str(doc))))\n",
        "                                metadata = doc.get(\"metadata\", {})\n",
        "                                doc_id = doc.get(\"id\")\n",
        "                                \n",
        "                                # Create Document object\n",
        "                                doc_obj = Document(\n",
        "                                    page_content=page_content,\n",
        "                                    metadata=metadata\n",
        "                                )\n",
        "                                if doc_id:\n",
        "                                    doc_obj.id = doc_id\n",
        "                                    \n",
        "                                documents.append(doc_obj)\n",
        "                    \n",
        "                    # Set documents as span outputs for RAG scorers\n",
        "                    if documents:\n",
        "                        span.set_outputs(documents)\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    # If parsing fails, log but don't break the flow\n",
        "                    print(f\"Warning: Could not parse retriever results: {e}\")\n",
        "            \n",
        "            return result\n",
        "\n",
        "    def call_llm(self, messages: list[dict[str, Any]]) -> Generator[dict[str, Any], None, None]:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
        "            for chunk in self.model_serving_client.chat.completions.create(\n",
        "                model=self.llm_endpoint,\n",
        "                messages=to_chat_completions_input(messages),\n",
        "                tools=self.get_tool_specs(),\n",
        "                stream=True,\n",
        "            ):\n",
        "                chunk_dict = chunk.to_dict()\n",
        "                if len(chunk_dict.get(\"choices\", [])) > 0:\n",
        "                    yield chunk_dict\n",
        "\n",
        "    def handle_tool_call(\n",
        "        self,\n",
        "        tool_call: dict[str, Any],\n",
        "        messages: list[dict[str, Any]],\n",
        "    ) -> ResponsesAgentStreamEvent:\n",
        "        \"\"\"\n",
        "        Execute tool calls, add them to the running message history, and return a ResponsesStreamEvent w/ tool output\n",
        "        \"\"\"\n",
        "        arguments_raw = tool_call.get(\"arguments\")\n",
        "        \n",
        "        # Handle different argument formats\n",
        "        if arguments_raw is None or arguments_raw == \"\":\n",
        "            args = {}\n",
        "        elif isinstance(arguments_raw, dict):\n",
        "            args = arguments_raw\n",
        "        elif isinstance(arguments_raw, str):\n",
        "            try:\n",
        "                args = json.loads(arguments_raw)\n",
        "            except json.JSONDecodeError as e:\n",
        "                # Try to extract the first valid JSON object if multiple are concatenated\n",
        "                try:\n",
        "                    # Find the first complete JSON object\n",
        "                    decoder = json.JSONDecoder()\n",
        "                    args, idx = decoder.raw_decode(arguments_raw)\n",
        "                except Exception as e2:\n",
        "                    args = {}\n",
        "        else:\n",
        "            args = {}\n",
        "        \n",
        "        result = str(self.execute_tool(tool_name=tool_call[\"name\"], args=args))\n",
        "\n",
        "        tool_call_output = self.create_function_call_output_item(tool_call[\"call_id\"], result)\n",
        "        messages.append(tool_call_output)\n",
        "        return ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=tool_call_output)\n",
        "\n",
        "    def call_and_run_tools(\n",
        "        self,\n",
        "        messages: list[dict[str, Any]],\n",
        "        max_iter: int = 10,\n",
        "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
        "        for _ in range(max_iter):\n",
        "            last_msg = messages[-1]\n",
        "            if last_msg.get(\"role\", None) == \"assistant\":\n",
        "                return\n",
        "            elif last_msg.get(\"type\", None) == \"function_call\":\n",
        "                yield self.handle_tool_call(last_msg, messages)\n",
        "            else:\n",
        "                yield from output_to_responses_items_stream(\n",
        "                    chunks=self.call_llm(messages), aggregator=messages\n",
        "                )\n",
        "\n",
        "        yield ResponsesAgentStreamEvent(\n",
        "            type=\"response.output_item.done\",\n",
        "            item=self.create_text_output_item(\"Max iterations reached. Stopping.\", str(uuid4())),\n",
        "        )\n",
        "\n",
        "    def _build_trace_metadata(self, request: ResponsesAgentRequest) -> dict[str, Any]:\n",
        "        metadata: dict[str, Any] = {}\n",
        "\n",
        "        session_id = None\n",
        "        if request.custom_inputs and \"session_id\" in request.custom_inputs:\n",
        "            session_id = request.custom_inputs.get(\"session_id\")\n",
        "        elif request.context and request.context.conversation_id:\n",
        "            session_id = request.context.conversation_id\n",
        "\n",
        "        if session_id:\n",
        "            metadata[\"mlflow.trace.session\"] = session_id\n",
        "\n",
        "        if request.custom_inputs and \"assistant_message_id\" in request.custom_inputs:\n",
        "            assistant_message_id = request.custom_inputs.get(\"assistant_message_id\")\n",
        "            if assistant_message_id:\n",
        "                metadata[\"app.assistant_message_id\"] = assistant_message_id\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def _apply_trace_metadata(self, request: ResponsesAgentRequest) -> None:\n",
        "        metadata = self._build_trace_metadata(request)\n",
        "        if metadata:\n",
        "            mlflow.update_current_trace(metadata=metadata)\n",
        "\n",
        "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
        "        self._apply_trace_metadata(request)\n",
        "\n",
        "        outputs = [\n",
        "            event.item\n",
        "            for event in self.predict_stream(request)\n",
        "            if event.type == \"response.output_item.done\"\n",
        "        ]\n",
        "        return ResponsesAgentResponse(output=outputs, custom_inputs=request.custom_inputs)\n",
        "\n",
        "    def predict_stream(self, request: ResponsesAgentRequest) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
        "        self._apply_trace_metadata(request)\n",
        "\n",
        "        messages = to_chat_completions_input([i.model_dump() for i in request.input])\n",
        "        if SYSTEM_PROMPT:\n",
        "            messages.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
        "        yield from self.call_and_run_tools(messages=messages)\n",
        "\n",
        "\n",
        "# Log the model using MLflow\n",
        "mlflow.openai.autolog()\n",
        "AGENT = ToolCallingAgent(llm_endpoint=LLM_ENDPOINT_NAME, tools=TOOL_INFOS)\n",
        "mlflow.models.set_model(AGENT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d06e9dea-e7b3-420e-b10c-08011f78116f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Prompt Registry Setup\n",
        "\n",
        "The agent has been refactored to use **MLflow Prompt Registry** for managing system prompts. This provides:\n",
        "\n",
        "* **Version Control**: Track changes to prompts over time\n",
        "* **Centralized Management**: Update prompts without modifying code\n",
        "* **Collaboration**: Share and iterate on prompts across teams\n",
        "* **Rollback**: Easily revert to previous prompt versions\n",
        "\n",
        "### How it works:\n",
        "\n",
        "1. **Register Prompt**: Create a prompt in Unity Catalog format (`catalog.schema.promptname`)\n",
        "2. **Agent Loads Prompt**: The agent automatically loads the latest version at runtime\n",
        "3. **Update Prompt**: Register new versions to iterate without redeploying the agent\n",
        "4. **Fallback**: If the prompt isn't found, the agent uses a default prompt\n",
        "\n",
        "### Prompt Name Format:\n",
        "\n",
        "In Databricks, prompts must be registered using Unity Catalog three-level naming:\n",
        "```\n",
        "catalog.schema.promptname\n",
        "```\n",
        "\n",
        "Example: `jack_demos_classic.data_catalogue_demo.catalog_rag_system_prompt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5b9de8b7-5466-43ff-a87d-03a07fd49792",
          "showTitle": true,
          "tableResultSettingsMap": {},
          "title": "Create initial prompt in registry"
        }
      },
      "outputs": [],
      "source": [
        "import mlflow.genai\n",
        "\n",
        "# Define the prompt name and initial template\n",
        "# In Databricks, prompts are registered in Unity Catalog format: catalog.schema.promptname\n",
        "prompt_name = \"jack_demos_classic.data_catalogue_demo.catalog_rag_system_prompt\"\n",
        "initial_prompt_template = \"\"\"you are a helpful assistant, when prompted with a query search for related entries in the vector search. You report back with tables and datasets related to the user question.\"\"\"\n",
        "\n",
        "# Create and register the prompt in MLflow prompt registry\n",
        "try:\n",
        "    # Try to register the prompt - if it exists, this will create a new version\n",
        "    print(f\"Registering prompt '{prompt_name}' in the registry...\")\n",
        "    prompt = mlflow.genai.register_prompt(\n",
        "        name=prompt_name,\n",
        "        template=initial_prompt_template,\n",
        "        commit_message=\"Initial system prompt for catalog RAG agent\",\n",
        "        tags={\n",
        "            \"task\": \"catalog_search\",\n",
        "            \"agent\": \"catalog_rag_agent\"\n",
        "        }\n",
        "    )\n",
        "    print(f\"âœ“ Prompt '{prompt.name}' (version {prompt.version}) registered successfully!\")\n",
        "    print(f\"Template: {initial_prompt_template}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error registering prompt: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3591a27b-35e0-47dd-aa89-1c8d534d87ab",
          "showTitle": true,
          "tableResultSettingsMap": {},
          "title": "Update prompt (optional)"
        }
      },
      "outputs": [],
      "source": [
        "# Optional: Update the prompt template in the registry\n",
        "# Uncomment and modify the template below to update the prompt\n",
        "\n",
        "# new_prompt_template = \"\"\"You are an expert data catalog assistant. When users ask about data, \n",
        "# search the vector index for relevant tables and datasets. Provide clear, structured responses \n",
        "# that include:\n",
        "# - Table names and their purpose\n",
        "# - Key columns and data types\n",
        "# - Relationships between tables\n",
        "# - Usage examples when relevant\n",
        "# \"\"\"\n",
        "\n",
        "# # Register a new version of the prompt\n",
        "# updated_prompt = mlflow.genai.register_prompt(\n",
        "#     name=\"jack_demos_classic.data_catalogue_demo.catalog_rag_system_prompt\",  # Must match the prompt name used in agent.py\n",
        "#     template=new_prompt_template,\n",
        "#     commit_message=\"Enhanced prompt with structured output format\"\n",
        "# )\n",
        "# print(f\"Prompt updated to version {updated_prompt.version}!\")\n",
        "\n",
        "print(\"To update the prompt, uncomment the code above and modify the template.\")\n",
        "print(\"Each update creates a new version in the prompt registry for version control.\")\n",
        "print(\"Note: In Databricks, prompts use Unity Catalog format: catalog.schema.promptname\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5b3e09bf-4ce2-45c4-894e-efeb9361c4c8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Test the agent\n",
        "\n",
        "Interact with the agent to test its output. Since we manually traced methods within `ResponsesAgent`, you can view the trace for each step the agent takes, with any LLM calls made via the OpenAI SDK automatically traced by autologging.\n",
        "\n",
        "Replace this placeholder input with an appropriate domain-specific example for your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "97a340be-1a90-45b1-9b3e-940fe59117e5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f8accaaa-545d-4bbc-9c77-9f7bd34ce8ce",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from agent import AGENT\n",
        "\n",
        "AGENT.predict(\n",
        "    {\"input\": [{\"role\": \"user\", \"content\": \"what is 4*3 in python\"}], \"custom_inputs\": {\"session_id\": \"test-session-123\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6d2b7172-9936-4328-88a4-947219bb18e6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "for chunk in AGENT.predict_stream(\n",
        "    {\"input\": [{\"role\": \"user\", \"content\": \"What is 4*3 in Python?\"}], \"custom_inputs\": {\"session_id\": \"test-session-123\"}}\n",
        "):\n",
        "    print(chunk.model_dump(exclude_none=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d4a1b30d-d079-49e6-9d54-caf1b31ad79b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Log the `agent` as an MLflow model\n",
        "Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
        "- **TODO**: If your Unity Catalog Function queries a [vector search index](https://docs.databricks.com/generative-ai/agent-framework/unstructured-retrieval-tools.html) or leverages [external functions](https://docs.databricks.com/generative-ai/agent-framework/external-connection-tools.html), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See [docs](https://docs.databricks.com/generative-ai/agent-framework/log-agent.html#specify-resources-for-automatic-authentication-passthrough) for more details.\n",
        "\n",
        "Log the agent as code from the `agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ab75aac6-d7be-484f-a3ea-73835e1c5dcc",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
        "import mlflow\n",
        "from agent import LLM_ENDPOINT_NAME, VECTOR_SEARCH_TOOLS, uc_toolkit\n",
        "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
        "from mlflow.tracking import MlflowClient\n",
        "from pkg_resources import get_distribution\n",
        "\n",
        "# Store runs in a shared experiment path.\n",
        "experiment_path = \"/Shared/rag_catalog_agent\"\n",
        "\n",
        "mlflow_client = MlflowClient()\n",
        "experiment = mlflow_client.get_experiment_by_name(experiment_path)\n",
        "if experiment is None:\n",
        "    experiment_id = mlflow_client.create_experiment(experiment_path)\n",
        "else:\n",
        "    experiment_id = experiment.experiment_id\n",
        "\n",
        "mlflow.set_experiment(experiment_id=experiment_id)\n",
        "print(f\"Using MLflow experiment: {experiment_path}\")\n",
        "\n",
        "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
        "for tool in VECTOR_SEARCH_TOOLS:\n",
        "    resources.extend(tool.resources)\n",
        "for tool in uc_toolkit.tools:\n",
        "    # TODO: If the UC function includes dependencies like external connection or vector search, please include them manually.\n",
        "    # See the TODO in the markdown above for more information.\n",
        "    udf_name = tool.get(\"function\", {}).get(\"name\", \"\").replace(\"__\", \".\")\n",
        "    resources.append(DatabricksFunction(function_name=udf_name))\n",
        "\n",
        "input_example = {\n",
        "    \"input\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"crm\"\n",
        "        }\n",
        "    ],\n",
        "    \"custom_inputs\": {\n",
        "        \"session_id\": \"test-session\"\n",
        "    }\n",
        "}\n",
        "\n",
        "with mlflow.start_run():\n",
        "    logged_agent_info = mlflow.pyfunc.log_model(\n",
        "        name=\"agent\",\n",
        "        python_model=\"agent.py\",\n",
        "        input_example=input_example,\n",
        "        pip_requirements=[\n",
        "            \"databricks-openai\",\n",
        "            \"backoff\",\n",
        "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
        "        ],\n",
        "        resources=resources,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d4ffd99d-0773-4ab6-a766-ef9586155274",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Evaluate the agent with [Agent Evaluation](https://docs.databricks.com/mlflow3/genai/eval-monitor)\n",
        "\n",
        "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
        "\n",
        "Evaluate your agent with one of our [predefined LLM scorers](https://docs.databricks.com/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://docs.databricks.com/mlflow3/genai/eval-monitor/custom-scorers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "823900a9-6281-457d-8ab8-14fd8e92860a",
          "showTitle": true,
          "tableResultSettingsMap": {},
          "title": "Cell 12"
        }
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from mlflow.genai.scorers import RelevanceToQuery, Safety, RetrievalRelevance, RetrievalGroundedness, Correctness\n",
        "\n",
        "eval_dataset = [\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"input\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Show me all tables related to CRM and sales opportunities\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        \"expectations\": {\n",
        "            \"expected_response\": \"should include the SALES_DB.CRM.OPPORTUNITIES table and related CRM tables\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"input\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"What tables contain financial data like invoices and general ledger entries?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        \"expectations\": {\n",
        "            \"expected_response\": \"should include FINANCE_DB.AP.INVOICES for accounts payable invoices and FINANCE_DB.GL.GENERAL_LEDGER for accounting journal entries\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"input\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"I need to analyze inventory levels by warehouse location\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        \"expectations\": {\n",
        "            \"expected_response\": \"should recommend SUPPLY_DB.WAREHOUSE.INVENTORY_SNAPSHOT which contains daily inventory levels by SKU and location\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"input\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Where can I find marketing campaign performance metrics and spend data?\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        \"expectations\": {\n",
        "            \"expected_response\": \"should reference MARKETING_DB.ATTRIBUTION.CAMPAIGN_PERFORMANCE which tracks daily campaign performance by channel including spend in USD\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "eval_results = mlflow.genai.evaluate(\n",
        "    data=eval_dataset,\n",
        "    predict_fn=lambda input: AGENT.predict({\"input\": input, \"custom_inputs\": {\"session_id\": \"evaluation-session\"}}),\n",
        "    scorers=[\n",
        "        RelevanceToQuery(),           # Response addresses the query\n",
        "        Safety(),                      # No harmful content\n",
        "        Correctness(),                 # Matches expected response\n",
        "        RetrievalRelevance(),          # Retrieved docs are relevant\n",
        "        RetrievalGroundedness()        # Response grounded in retrieved context\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Review the evaluation results in the MLflow UI (see console output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "095fb21b-29b9-4387-ada8-2a780b62e524",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Optimize the System Prompt\n",
        "\n",
        "Use MLflow's **Prompt Optimization** to automatically improve your agent's system prompt based on evaluation metrics. This uses the GEPA (Generative Prompt Adaptation) algorithm to iteratively refine prompts through LLM-driven reflection and automated feedback.\n",
        "\n",
        "### How it works:\n",
        "\n",
        "1. **Analyzes Performance**: Reviews your agent's outputs on the evaluation dataset\n",
        "2. **Identifies Issues**: Uses LLM reflection to understand where the prompt falls short\n",
        "3. **Generates Improvements**: Creates an optimized prompt that addresses the issues\n",
        "4. **Validates**: Tests the new prompt against your evaluation metrics\n",
        "5. **Registers**: Automatically saves the optimized prompt as a new version in the registry\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "* **Data-Driven**: Uses your actual evaluation data to guide optimization\n",
        "* **Automatic**: No manual prompt engineering required\n",
        "* **Version Controlled**: New prompt versions are tracked in the registry\n",
        "* **Measurable**: Shows improvement in evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5954e470-c937-4cdf-8fda-76ff22c57a0e",
          "showTitle": true,
          "tableResultSettingsMap": {},
          "title": "Run Prompt Optimization"
        }
      },
      "outputs": [],
      "source": [
        "from mlflow.genai.optimize import GepaPromptOptimizer\n",
        "from mlflow.genai.scorers import RelevanceToQuery, Correctness\n",
        "import mlflow.genai\n",
        "from agent import PROMPT_NAME\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# Get the latest version number\n",
        "client = MlflowClient()\n",
        "response = client.search_prompt_versions(PROMPT_NAME)\n",
        "latest_version = max(v.version for v in response.prompt_versions)\n",
        "\n",
        "# Define a predict function that explicitly loads and formats the prompt\n",
        "def predict_with_prompt(input):\n",
        "    \"\"\"Prediction function that loads the prompt from registry and uses it.\"\"\"\n",
        "    # Load the specific version of the prompt that we're optimizing\n",
        "    prompt = mlflow.genai.load_prompt(f\"prompts:/{PROMPT_NAME}/{latest_version}\")\n",
        "    \n",
        "    # Call format() to signal to the optimizer that this prompt is being used\n",
        "    _ = prompt.format()\n",
        "    \n",
        "    # Now call the agent which will load and use the prompt internally\n",
        "    return AGENT.predict({\n",
        "        \"input\": input,\n",
        "        \"custom_inputs\": {\"session_id\": \"optimization-session\"}\n",
        "    })\n",
        "\n",
        "# Prepare training data in the format expected by optimize_prompts\n",
        "train_data = [\n",
        "    {\n",
        "        \"inputs\": item[\"inputs\"],\n",
        "        \"expectations\": item[\"expectations\"]\n",
        "    }\n",
        "    for item in eval_dataset\n",
        "]\n",
        "\n",
        "print(f\"Optimizing prompt: {PROMPT_NAME}\")\n",
        "print(f\"Current version: {latest_version}\")\n",
        "print(f\"Training data size: {len(train_data)} examples\")\n",
        "print(\"\\nStarting prompt optimization... This may take several minutes.\\n\")\n",
        "\n",
        "# Run prompt optimization\n",
        "# Note: Only using scorers that return single numerical values\n",
        "# RetrievalRelevance and RetrievalGroundedness return lists and cannot be used for optimization\n",
        "optimization_result = mlflow.genai.optimize_prompts(\n",
        "    predict_fn=predict_with_prompt,\n",
        "    train_data=train_data,\n",
        "    prompt_uris=[f\"prompts:/{PROMPT_NAME}/{latest_version}\"],\n",
        "    optimizer=GepaPromptOptimizer(\n",
        "        reflection_model=\"databricks:/databricks-gpt-5\",\n",
        "        max_metric_calls=10,\n",
        "    ),\n",
        "    scorers=[\n",
        "        RelevanceToQuery(),\n",
        "        Correctness(),\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OPTIMIZATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOptimized prompt has been registered as a new version in: {PROMPT_NAME}\")\n",
        "print(f\"\\nOriginal prompt version: {latest_version}\")\n",
        "\n",
        "if hasattr(optimization_result, 'metrics'):\n",
        "    print(f\"\\nPerformance improvement:\")\n",
        "    for metric_name, scores in optimization_result.metrics.items():\n",
        "        if isinstance(scores, dict) and 'optimized' in scores and 'original' in scores:\n",
        "            print(f\"  {metric_name}: {scores['optimized']:.3f} (was {scores['original']:.3f})\")\n",
        "        else:\n",
        "            print(f\"  {metric_name}: {scores}\")\n",
        "\n",
        "if hasattr(optimization_result, 'optimized_prompt'):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPTIMIZED PROMPT\")\n",
        "    print(\"=\"*80)\n",
        "    print(optimization_result.optimized_prompt)\n",
        "    print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b4c15720-5aa2-4f4d-8c58-8c8253c67483",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Perform pre-deployment validation of the agent\n",
        "Before registering and deploying the agent, we perform pre-deployment checks via the [mlflow.models.predict()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict) API. See [documentation](https://docs.databricks.com/machine-learning/model-serving/model-serving-debug.html#validate-inputs) for details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8792365f-6612-4e96-8e2e-3e5796dbfab4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "mlflow.models.predict(\n",
        "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
        "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}], \"custom_inputs\": {\"session_id\": \"validation-session\"}},\n",
        "    env_manager=\"uv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "746c9832-d34e-4df6-b5b2-2f8e54ce268d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Register the model to Unity Catalog\n",
        "\n",
        "Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "12e0c87f-e475-4b22-bf4a-c4fafc24817e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "# TODO: define the catalog, schema, and model name for your UC model\n",
        "catalog = \"jack_demos_classic\"\n",
        "schema = \"data_catalogue_demo\"\n",
        "model_name = \"catalog_rag_agent\"\n",
        "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
        "\n",
        "# register the model to UC\n",
        "uc_registered_model_info = mlflow.register_model(\n",
        "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b9f4a3f1-dc1a-468b-96bc-2ec9389d2fb8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Deploy the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "891fcb2a-d28a-4393-bdc6-8098b8514308",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from databricks import agents\n",
        "# NOTE: pass scale_to_zero=True to agents.deploy() to enable scale-to-zero for cost savings.\n",
        "# This is not recommended for production workloads, as capacity is not guaranteed when scaled to zero.\n",
        "# Scaled to zero endpoints may take extra time to respond when queried, while they scale back up.\n",
        "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"playground\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7e360dbb-d1cc-49c0-b018-fb3bfa9bd059",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Next steps\n",
        "\n",
        "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See [docs](https://docs.databricks.com/generative-ai/deploy-agent.html) for details"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "driver",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
