{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy Vector Search After Pipeline Run\n",
        "\n",
        "This notebook provisions a Vector Search endpoint and Delta Sync index for `metadata_docs` after your Lakeflow pipeline completes.\n",
        "\n",
        "It does the following:\n",
        "1. Validates the source table exists and has rows.\n",
        "2. Creates (or reuses) a Vector Search endpoint.\n",
        "3. Creates (or reuses) a Delta Sync index with managed embeddings.\n",
        "4. Triggers sync (for `TRIGGERED` pipeline type).\n",
        "5. Runs a similarity search sanity check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional package install\n",
        "\n",
        "If your cluster does not already have Vector Search SDK installed, run this cell once and then restart Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U databricks-vectorsearch databricks-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94208539",
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks.vector_search.client import VectorSearchClient\n",
        "from pyspark.sql import functions as F\n",
        "import time\n",
        "\n",
        "# ---------- Config ----------\n",
        "dbutils.widgets.text(\"catalog\", \"jack_demos_classic\", \"UC Catalog\")\n",
        "dbutils.widgets.text(\"schema\", \"data_catalogue_demo\", \"UC Schema\")\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\").strip()\n",
        "schema = dbutils.widgets.get(\"schema\").strip()\n",
        "\n",
        "if not catalog or not schema:\n",
        "    raise ValueError(\"Both 'catalog' and 'schema' widget values are required.\")\n",
        "\n",
        "source_table = f\"{catalog}.{schema}.metadata_docs\"\n",
        "index_name = f\"{catalog}.{schema}.metadata_docs_index\"\n",
        "\n",
        "# Use a stable endpoint name if you have one already.\n",
        "endpoint_name = \"metadata-catalogue-vs-endpoint\"\n",
        "endpoint_type = \"STANDARD\"  # or \"STORAGE_OPTIMIZED\"\n",
        "\n",
        "# Managed embedding model endpoint\n",
        "embedding_model_endpoint = \"databricks-gte-large-en\"\n",
        "pipeline_type = \"TRIGGERED\"  # or CONTINUOUS\n",
        "\n",
        "display({\n",
        "    \"catalog\": catalog,\n",
        "    \"schema\": schema,\n",
        "    \"source_table\": source_table,\n",
        "    \"index_name\": index_name,\n",
        "    \"endpoint_name\": endpoint_name,\n",
        "    \"embedding_model_endpoint\": embedding_model_endpoint,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Validate source table ----------\n",
        "df = spark.table(source_table)\n",
        "required_cols = {\"doc_id\", \"content\"}\n",
        "missing = required_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Source table missing required columns: {missing}\")\n",
        "\n",
        "row_count = df.count()\n",
        "if row_count == 0:\n",
        "    raise ValueError(f\"{source_table} has 0 rows. Run/refresh pipeline first.\")\n",
        "\n",
        "print(f\"Source table validated: {source_table} ({row_count} rows)\")\n",
        "display(df.select(\"doc_id\", \"full_table_name\", \"content\").limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vsc = VectorSearchClient()\n",
        "\n",
        "def endpoint_state(endpoint_payload: dict) -> str:\n",
        "    # API shape can vary by SDK/runtime version.\n",
        "    for path in [\n",
        "        (\"endpoint_status\", \"state\"),\n",
        "        (\"status\", \"state\"),\n",
        "        (\"state\",),\n",
        "    ]:\n",
        "        cur = endpoint_payload\n",
        "        ok = True\n",
        "        for p in path:\n",
        "            if isinstance(cur, dict) and p in cur:\n",
        "                cur = cur[p]\n",
        "            else:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok and cur:\n",
        "            return str(cur)\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "# ---------- Create/reuse endpoint ----------\n",
        "all_eps = vsc.list_endpoints()\n",
        "eps = all_eps.get(\"endpoints\", []) if isinstance(all_eps, dict) else []\n",
        "exists = any(e.get(\"name\") == endpoint_name for e in eps)\n",
        "\n",
        "if not exists:\n",
        "    print(f\"Creating endpoint: {endpoint_name} ({endpoint_type})\")\n",
        "    vsc.create_endpoint(name=endpoint_name, endpoint_type=endpoint_type)\n",
        "else:\n",
        "    print(f\"Endpoint already exists: {endpoint_name}\")\n",
        "\n",
        "for i in range(60):\n",
        "    ep = vsc.get_endpoint(endpoint_name)\n",
        "    state = endpoint_state(ep if isinstance(ep, dict) else {})\n",
        "    print(f\"Endpoint state: {state}\")\n",
        "    if state.upper() in {\"ONLINE\", \"READY\"}:\n",
        "        break\n",
        "    if state.upper() in {\"FAILED\", \"ERROR\"}:\n",
        "        raise RuntimeError(f\"Endpoint failed: {ep}\")\n",
        "    time.sleep(10)\n",
        "else:\n",
        "    raise TimeoutError(\"Endpoint did not become ready in time\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Create/reuse Delta Sync index ----------\n",
        "idxs = vsc.list_indexes(endpoint_name)\n",
        "idx_list = idxs.get(\"vector_indexes\", []) if isinstance(idxs, dict) else []\n",
        "index_exists = any(i.get(\"name\") == index_name for i in idx_list)\n",
        "\n",
        "if not index_exists:\n",
        "    print(f\"Creating index: {index_name}\")\n",
        "    try:\n",
        "        # Older SDK shape\n",
        "        vsc.create_delta_sync_index(\n",
        "            endpoint_name=endpoint_name,\n",
        "            index_name=index_name,\n",
        "            source_table_name=source_table,\n",
        "            primary_key=\"doc_id\",\n",
        "            pipeline_type=pipeline_type,\n",
        "            embedding_source_column=\"content\",\n",
        "            embedding_model_endpoint_name=embedding_model_endpoint,\n",
        "        )\n",
        "    except TypeError:\n",
        "        # Newer SDK shape\n",
        "        vsc.create_delta_sync_index(\n",
        "            endpoint_name=endpoint_name,\n",
        "            index_name=index_name,\n",
        "            source_table_name=source_table,\n",
        "            primary_key=\"doc_id\",\n",
        "            pipeline_type=pipeline_type,\n",
        "            embedding_source_columns=[\n",
        "                {\n",
        "                    \"name\": \"content\",\n",
        "                    \"embedding_model_endpoint_name\": embedding_model_endpoint,\n",
        "                }\n",
        "            ],\n",
        "        )\n",
        "else:\n",
        "    print(f\"Index already exists: {index_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Sync (TRIGGERED) + readiness wait ----------\n",
        "try:\n",
        "    index = vsc.get_index(endpoint_name=endpoint_name, index_name=index_name)\n",
        "except TypeError:\n",
        "    index = vsc.get_index(index_name=index_name, endpoint_name=endpoint_name)\n",
        "\n",
        "if pipeline_type.upper() == \"TRIGGERED\":\n",
        "    print(\"Triggering index sync...\")\n",
        "    try:\n",
        "        index.sync()\n",
        "    except Exception:\n",
        "        # Some SDKs expose sync on client instead of handle.\n",
        "        vsc.sync_index(index_name=index_name)\n",
        "\n",
        "def index_state(payload: dict) -> str:\n",
        "    for path in [\n",
        "        (\"status\", \"detailed_state\"),\n",
        "        (\"status\", \"state\"),\n",
        "        (\"detailed_state\",),\n",
        "        (\"state\",),\n",
        "    ]:\n",
        "        cur = payload\n",
        "        ok = True\n",
        "        for p in path:\n",
        "            if isinstance(cur, dict) and p in cur:\n",
        "                cur = cur[p]\n",
        "            else:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok and cur:\n",
        "            return str(cur)\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "for i in range(90):\n",
        "    d = index.describe()\n",
        "    s = index_state(d if isinstance(d, dict) else {})\n",
        "    print(f\"Index state: {s}\")\n",
        "    upper_s = s.upper()\n",
        "    if any(k in upper_s for k in [\"ONLINE\", \"READY\", \"SUCCESS\"]):\n",
        "        break\n",
        "    if any(k in upper_s for k in [\"FAILED\", \"ERROR\"]):\n",
        "        raise RuntimeError(f\"Index failed: {d}\")\n",
        "    time.sleep(10)\n",
        "else:\n",
        "    raise TimeoutError(\"Index did not become ready in time\")\n",
        "\n",
        "print(\"Index is ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Similarity search sanity check ----------\n",
        "query = \"Find tables with inventory or shipment information\"\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "results = index.similarity_search(\n",
        "    query_text=query,\n",
        "    columns=[\"doc_id\", \"full_table_name\", \"content\"],\n",
        "    num_results=5,\n",
        ")\n",
        "\n",
        "display(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
